{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "json_dir = '/root/notebooks/0858611-2/tooth_xray/model/all_tooth_data_set/mapping.json'\n",
    "with open(json_dir) as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_level(mean):\n",
    "    if mean>=0 and mean<=2:\n",
    "        return 'normal'\n",
    "    elif mean>=3 and mean<=4:\n",
    "        return 'medium'\n",
    "    elif mean>4:\n",
    "        return 'serious'\n",
    "    else:\n",
    "        return 'over_range'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset/Images/037621_12_.png None\n",
      "Dataset/Images/037621_13_.png None\n",
      "Dataset/Images/037621_26_.png None\n",
      "Dataset/Images/037621_25_.png None\n",
      "Dataset/Images/039462_14_.png None\n",
      "Dataset/Images/039462_22_.png None\n",
      "Dataset/Images/039462_21_.png None\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "import pandas as pd\n",
    "img_path = []\n",
    "img_cal = []\n",
    "img_cal_mean = []\n",
    "img_cal_level = []\n",
    "\n",
    "for key in json_data:\n",
    "    try:\n",
    "        new_path = key.replace(\"Dataset/Images\", \"/root/notebooks/0858611-2/tooth_xray/model/all_tooth_data_set/Images\")\n",
    "        mean_cal = int(mean(json_data[key]))\n",
    "        img_cal_level.append(get_level(mean_cal))\n",
    "        img_cal_mean.append(str(mean_cal))\n",
    "        img_path.append(new_path)\n",
    "        img_cal.append(json_data[key])\n",
    "    except:\n",
    "        print(key,json_data[key])\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506\n",
      "1506\n",
      "1506\n",
      "1506\n",
      "699\n",
      "346\n",
      "109\n"
     ]
    }
   ],
   "source": [
    "print(len(img_path))\n",
    "print(len(img_cal))\n",
    "print(len(img_cal_mean))\n",
    "print(len(img_cal_level))\n",
    "print(img_cal_level.count('normal'))\n",
    "print(img_cal_level.count('medium'))\n",
    "print(img_cal_level.count('serious'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          image_path img_cal img_cal_mean  \\\n",
      "0  /root/notebooks/0858611-2/tooth_xray/model/all...  [5, 3]            4   \n",
      "1  /root/notebooks/0858611-2/tooth_xray/model/all...  [3, 4]            3   \n",
      "2  /root/notebooks/0858611-2/tooth_xray/model/all...  [7, 9]            8   \n",
      "3  /root/notebooks/0858611-2/tooth_xray/model/all...  [4, 5]            4   \n",
      "4  /root/notebooks/0858611-2/tooth_xray/model/all...  [4, 3]            3   \n",
      "\n",
      "  img_cal_level  \n",
      "0        medium  \n",
      "1        medium  \n",
      "2       serious  \n",
      "3        medium  \n",
      "4        medium  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dict = {\"image_path\": img_path,  \n",
    "        \"img_cal\": img_cal,\n",
    "        \"img_cal_mean\":img_cal_mean,\n",
    "        \"img_cal_level\":img_cal_level\n",
    "       }\n",
    "\n",
    "tooth_dataframe = pd.DataFrame(dict)\n",
    "print(tooth_dataframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tooth_dataframe = tooth_dataframe[tooth_dataframe.img_cal_level != 'over_range']\n",
    "\n",
    "normal_dataframe = tooth_dataframe[tooth_dataframe.img_cal_level == 'normal']\n",
    "medium_dataframe = tooth_dataframe[tooth_dataframe.img_cal_level == 'medium']\n",
    "serious_dataframe = tooth_dataframe[tooth_dataframe.img_cal_level == 'serious']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1154, 256, 256, 3)\n",
      "(1154,)\n"
     ]
    }
   ],
   "source": [
    "#img to array and resize \n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def imgpath_list_to_array_list(imgpath_list,imgsize=(256,256)):\n",
    "    array_list = []\n",
    "    for img_path in imgpath_list:\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img,imgsize)\n",
    "        #img = (img-127.5)/127.5\n",
    "        array_list.append(img)\n",
    "    return np.array(array_list)\n",
    "\n",
    "def label_list_to_num(train_label):\n",
    "    label_array = []\n",
    "    for symptom in train_label:\n",
    "        if symptom == 'normal':\n",
    "            label_array.append(0)\n",
    "        if symptom == 'medium':\n",
    "            label_array.append(1)\n",
    "        if symptom == 'serious':\n",
    "            label_array.append(2)\n",
    "    return np.array(label_array)\n",
    "\n",
    "train_data_array = imgpath_list_to_array_list(tooth_dataframe.image_path)\n",
    "train_label_array = label_list_to_num(tooth_dataframe.img_cal_level)\n",
    "\n",
    "normal_data_array = imgpath_list_to_array_list(normal_dataframe.image_path)\n",
    "normal_label_array = label_list_to_num(normal_dataframe.img_cal_level)\n",
    "medium_data_array = imgpath_list_to_array_list(medium_dataframe.image_path)\n",
    "medium_label_array= label_list_to_num(medium_dataframe.img_cal_level)\n",
    "serious_data_array = imgpath_list_to_array_list(serious_dataframe.image_path)\n",
    "serious_label_array = label_list_to_num(serious_dataframe.img_cal_level)\n",
    "\n",
    "\n",
    "print(train_data_array.shape)\n",
    "print(train_label_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input,Dropout\n",
    "\n",
    "\n",
    "def my_convolution_model(input_shape, classes):\n",
    "    inputs=Input(input_shape)\n",
    "    x = Conv2D(32, (3, 3), activation='relu')(inputs)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(2048, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(classes)(x)\n",
    "    model = Model(inputs, x)\n",
    "    model.compile(optimizer='adam',\n",
    "          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "          metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "input_shape= (256,256,3)\n",
    "classes    = 3\n",
    "batch_size = 256\n",
    "epochs     = 30\n",
    "\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
    "with mirrored_strategy.scope():\n",
    "    model_convolution = my_convolution_model(input_shape, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_convolution.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 154 samples\n",
      "Epoch 1/30\n",
      "INFO:tensorflow:batch_all_reduce: 16 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 16 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "1000/1000 [==============================] - 18s 18ms/sample - loss: 204.6900 - accuracy: 0.3540 - val_loss: 4.8721 - val_accuracy: 0.6948\n",
      "Epoch 2/30\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 2.3018 - accuracy: 0.5070 - val_loss: 1.0921 - val_accuracy: 0.6948\n",
      "Epoch 3/30\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.9710 - accuracy: 0.5530 - val_loss: 0.9854 - val_accuracy: 0.6948\n",
      "Epoch 4/30\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.9439 - accuracy: 0.5850 - val_loss: 1.0783 - val_accuracy: 0.6948\n",
      "Epoch 5/30\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.9366 - accuracy: 0.5770 - val_loss: 0.9960 - val_accuracy: 0.6883\n",
      "Epoch 6/30\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.9222 - accuracy: 0.5880 - val_loss: 0.9809 - val_accuracy: 0.6688\n",
      "Epoch 7/30\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.9108 - accuracy: 0.5830 - val_loss: 0.9611 - val_accuracy: 0.6558\n",
      "Epoch 8/30\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.8984 - accuracy: 0.5740 - val_loss: 0.9838 - val_accuracy: 0.6429\n",
      "Epoch 9/30\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.8737 - accuracy: 0.5680 - val_loss: 0.9881 - val_accuracy: 0.6364\n",
      "Epoch 10/30\n",
      "1000/1000 [==============================] - 1s 909us/sample - loss: 0.8495 - accuracy: 0.5820 - val_loss: 0.9690 - val_accuracy: 0.6753\n",
      "Epoch 11/30\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.8426 - accuracy: 0.5900 - val_loss: 0.9547 - val_accuracy: 0.6558\n",
      "Epoch 12/30\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.8109 - accuracy: 0.6100 - val_loss: 0.9501 - val_accuracy: 0.6429\n",
      "Epoch 13/30\n",
      "1000/1000 [==============================] - 1s 911us/sample - loss: 0.7755 - accuracy: 0.6220 - val_loss: 0.9689 - val_accuracy: 0.6558\n",
      "Epoch 14/30\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.7544 - accuracy: 0.6450 - val_loss: 0.9320 - val_accuracy: 0.6494\n",
      "Epoch 15/30\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: 0.7332 - accuracy: 0.6720 - val_loss: 0.9929 - val_accuracy: 0.5390\n",
      "Epoch 16/30\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.7348 - accuracy: 0.6410 - val_loss: 0.9771 - val_accuracy: 0.6299\n",
      "Epoch 17/30\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.6900 - accuracy: 0.6640 - val_loss: 1.0599 - val_accuracy: 0.6364\n",
      "Epoch 18/30\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.6416 - accuracy: 0.6980 - val_loss: 1.0328 - val_accuracy: 0.6558\n",
      "Epoch 19/30\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.6205 - accuracy: 0.7140 - val_loss: 1.2134 - val_accuracy: 0.6104\n",
      "Epoch 20/30\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: 0.5317 - accuracy: 0.7530 - val_loss: 1.1615 - val_accuracy: 0.6688\n",
      "Epoch 21/30\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.5012 - accuracy: 0.7830 - val_loss: 1.2977 - val_accuracy: 0.6948\n",
      "Epoch 22/30\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.4364 - accuracy: 0.8180 - val_loss: 1.3573 - val_accuracy: 0.5714\n",
      "Epoch 23/30\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: 0.3772 - accuracy: 0.8550 - val_loss: 1.3796 - val_accuracy: 0.6364\n",
      "Epoch 24/30\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: 0.3334 - accuracy: 0.8610 - val_loss: 1.6024 - val_accuracy: 0.6234\n",
      "Epoch 25/30\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.3002 - accuracy: 0.8830 - val_loss: 1.4577 - val_accuracy: 0.6364\n",
      "Epoch 26/30\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.2548 - accuracy: 0.9070 - val_loss: 1.8719 - val_accuracy: 0.6169\n",
      "Epoch 27/30\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.2693 - accuracy: 0.8970 - val_loss: 1.5983 - val_accuracy: 0.5714\n",
      "Epoch 28/30\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.2262 - accuracy: 0.9230 - val_loss: 1.8090 - val_accuracy: 0.5649\n",
      "Epoch 29/30\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.1836 - accuracy: 0.9390 - val_loss: 2.8189 - val_accuracy: 0.4870\n",
      "Epoch 30/30\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.1580 - accuracy: 0.9390 - val_loss: 2.1714 - val_accuracy: 0.5909\n"
     ]
    }
   ],
   "source": [
    "history = model_convolution.fit(train_data_array[:1000], train_label_array[:1000], epochs=epochs, \n",
    "                     validation_data=(train_data_array[1000:], train_label_array[1000:]),shuffle=True,batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpredict = my_convolution_model(input_shape, classes)\n",
    "\n",
    "weights = model_convolution.get_weights()\n",
    "modelpredict.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/1 - 1s - loss: 1.8123 - accuracy: 0.5909\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model_convolution.evaluate(train_data_array[1000:],  train_label_array[1000:], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/1 - 1s - loss: 1.8123 - accuracy: 0.5909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.171412365777152, 0.59090906]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_convolution.evaluate(train_data_array[1000:],  train_label_array[1000:], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "following is recall rate\n",
      "\n",
      "# Evaluate on normal_data\n",
      "699/1 - 1s - loss: 0.8040 - accuracy: 0.9599\n",
      "\n",
      "# Evaluate on medium_data\n",
      "346/1 - 1s - loss: 1.0510 - accuracy: 0.9306\n",
      "\n",
      "# Evaluate on serious_data\n",
      "109/1 - 1s - loss: 3.6978 - accuracy: 0.7798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7999304325208751, 0.7798165]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('following is recall rate')\n",
    "print('\\n# Evaluate on normal_data')\n",
    "model_convolution.evaluate(normal_data_array,normal_label_array, verbose=2)\n",
    "\n",
    "print('\\n# Evaluate on medium_data')\n",
    "model_convolution.evaluate(medium_data_array,medium_label_array, verbose=2)\n",
    "\n",
    "print('\\n# Evaluate on serious_data')\n",
    "model_convolution.evaluate(serious_data_array,serious_label_array, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "def recall_accuracy_rate(test_data_array,test_label_array,model):\n",
    "    model_predict =[]\n",
    "    for img in test_data_array:\n",
    "        reshape = img.reshape(1,256,256,3)\n",
    "        image = tf.cast(reshape, tf.float32)\n",
    "        model_predict.append(np.argmax(model.predict(image)))\n",
    "        \n",
    "    matrix = confusion_matrix(test_label_array, model_predict)\n",
    "    matrix_split = np.split(matrix,3,axis=1)\n",
    "    print('column:* is prediction result')\n",
    "    print('row*: is real class')\n",
    "    dict = {#\"\": img_path,\n",
    "            \"class:0\":matrix_split[0].reshape(3),\n",
    "            \"class:1\":matrix_split[1].reshape(3),\n",
    "            \"class:2\":matrix_split[2].reshape(3)\n",
    "           }\n",
    "    \n",
    "    predict_table = pd.DataFrame(dict)\n",
    "    return predict_table,matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(label, confusion_matrix):\n",
    "    col = confusion_matrix[:, label]\n",
    "    return confusion_matrix[label, label] / col.sum()\n",
    "    \n",
    "def recall(label, confusion_matrix):\n",
    "    row = confusion_matrix[label, :]\n",
    "    return confusion_matrix[label, label] / row.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column:* is prediction result\n",
      "row*: is real class\n",
      "The class:0 precision is 0.7642 and recall is 0.7570 in \n",
      "The class:1 precision is 0.1463 and recall is 0.3158 in \n",
      "The class:2 precision is 0.5714 and recall is 0.1429 in \n"
     ]
    }
   ],
   "source": [
    "predict_table,matrix = recall_accuracy_rate(train_data_array[1000:],train_label_array[1000:],modelpredict)\n",
    "for i in range(len(predict_table.columns)):\n",
    "    p = precision(i,matrix)\n",
    "    r = recall(i,matrix) \n",
    "    print('The {} precision is {:.4f} and recall is {:.4f} in '.format(predict_table.columns[i],p,r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class:0</th>\n",
       "      <th>class:1</th>\n",
       "      <th>class:2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class:0  class:1  class:2\n",
       "0       81       24        2\n",
       "1       12        6        1\n",
       "2       13       11        4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
