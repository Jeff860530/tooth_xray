{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "Num GPUs Available:  2\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    print('error')\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///root/notebooks/0858611-2/tf-explain\n",
      "Requirement already satisfied: opencv-python>=4.1.0.25 in /opt/conda/lib/python3.6/site-packages (from tf-explain==0.2.1) (4.1.2.30)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.6/site-packages (from opencv-python>=4.1.0.25->tf-explain==0.2.1) (1.18.3)\n",
      "Installing collected packages: tf-explain\n",
      "  Found existing installation: tf-explain 0.2.1\n",
      "    Uninstalling tf-explain-0.2.1:\n",
      "      Successfully uninstalled tf-explain-0.2.1\n",
      "  Running setup.py develop for tf-explain\n",
      "Successfully installed tf-explain\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tf_explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Image_path  label\n",
      "0  /root/notebooks/0858611-2/tooth_xray/model/700...      2\n",
      "1  /root/notebooks/0858611-2/tooth_xray/model/700...      1\n",
      "2  /root/notebooks/0858611-2/tooth_xray/model/700...      2\n",
      "3  /root/notebooks/0858611-2/tooth_xray/model/700...      2\n",
      "4  /root/notebooks/0858611-2/tooth_xray/model/700...      1\n"
     ]
    }
   ],
   "source": [
    "#prepare training data\n",
    "import pandas as pd\n",
    "import cv2\n",
    "train_csv_path = '/root/notebooks/0858611-2/tooth_xray/model/700_700/image_generate_all_rotate/train_label.csv'\n",
    "train_tooth_dataframe = pd.read_csv (train_csv_path)\n",
    "#tooth_dataframe = tooth_dataframe.sort_values(by=['Image_path'][-35:])\n",
    "print (train_tooth_dataframe.head())\n",
    "\n",
    "train_img_path_list = train_tooth_dataframe.Image_path\n",
    "train_img_label_list = train_tooth_dataframe.label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def imgpath_list_to_array_list(imgpath_list,imgsize=(256,256)):\n",
    "    array_list = []\n",
    "    for img_path in imgpath_list:\n",
    "        img = cv2.imread(img_path,0)\n",
    "        img = cv2.resize(img,imgsize)\n",
    "        #img = img[:,:,0]\n",
    "        #img = (img-127.5)/127.5\n",
    "        array_list.append(img.reshape(256,256,1))\n",
    "    return np.array(array_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Image_path  label\n",
      "0  /root/notebooks/0858611-2/tooth_xray/model/700...      0\n",
      "1  /root/notebooks/0858611-2/tooth_xray/model/700...      0\n",
      "2  /root/notebooks/0858611-2/tooth_xray/model/700...      2\n",
      "3  /root/notebooks/0858611-2/tooth_xray/model/700...      1\n",
      "4  /root/notebooks/0858611-2/tooth_xray/model/700...      2\n"
     ]
    }
   ],
   "source": [
    "#prepare test data\n",
    "import pandas as pd\n",
    "test_csv_path = '/root/notebooks/0858611-2/tooth_xray/model/700_700/image_generate_all_rotate/test_label.csv'\n",
    "test_tooth_dataframe = pd.read_csv (test_csv_path)\n",
    "#tooth_dataframe = tooth_dataframe.sort_values(by=['Image_path'][-35:])\n",
    "print (test_tooth_dataframe.head())\n",
    "test_img_path_list = test_tooth_dataframe.Image_path\n",
    "test_img_label_list = test_tooth_dataframe.label\n",
    "\n",
    "#test_data_array = imgpath_list_to_array_list(test_tooth_dataframe.Image_path)\n",
    "#test_label_array = np.array(test_tooth_dataframe.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12400\n",
      "8000\n",
      "3600\n"
     ]
    }
   ],
   "source": [
    "normal_dataframe = train_tooth_dataframe[train_tooth_dataframe.label == 0]\n",
    "medium_dataframe = train_tooth_dataframe[train_tooth_dataframe.label == 1]\n",
    "serious_dataframe = train_tooth_dataframe[train_tooth_dataframe.label == 2]\n",
    "print(len(normal_dataframe))\n",
    "print(len(medium_dataframe))\n",
    "print(len(serious_dataframe))\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "normal_dataframe = shuffle(normal_dataframe)\n",
    "medium_dataframe = shuffle(medium_dataframe)\n",
    "serious_dataframe = shuffle(serious_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600\n"
     ]
    }
   ],
   "source": [
    "min_len = min(len(normal_dataframe),len(medium_dataframe),len(serious_dataframe))\n",
    "print(min_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10800\n",
      "                                              Image_path  label\n",
      "10465  /root/notebooks/0858611-2/tooth_xray/model/700...      2\n",
      "3202   /root/notebooks/0858611-2/tooth_xray/model/700...      1\n",
      "10721  /root/notebooks/0858611-2/tooth_xray/model/700...      2\n",
      "11818  /root/notebooks/0858611-2/tooth_xray/model/700...      0\n",
      "17874  /root/notebooks/0858611-2/tooth_xray/model/700...      1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "train_tooth_dataframe = pd.concat([normal_dataframe[:min_len],medium_dataframe[:min_len],serious_dataframe[:min_len]],axis=0)\n",
    "print(len(train_tooth_dataframe))\n",
    "train_tooth_dataframe = shuffle(train_tooth_dataframe)\n",
    "print(train_tooth_dataframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img to array and resize \n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "imgsize = (256,256)\n",
    "\n",
    "def imgpath_list_to_array_list(imgpath_list,imgsize=(256,256)):\n",
    "    array_list = []\n",
    "    for img_path in imgpath_list:\n",
    "        img = cv2.imread(img_path,0)\n",
    "        img = cv2.resize(img,imgsize)\n",
    "        #img = img[:,:,0]\n",
    "        #img = (img-127.5)/127.5\n",
    "        array_list.append(img.reshape(256,256,1))\n",
    "    return np.array(array_list)\n",
    "\n",
    "# def label_list_to_num(train_label):\n",
    "#     label_array = []\n",
    "#     for symptom in train_label:\n",
    "#         if symptom == 'normal':\n",
    "#             label_array.append(0)\n",
    "#         if symptom == 'medium':\n",
    "#             label_array.append(1)\n",
    "#         if symptom == 'serious':\n",
    "#             label_array.append(2)\n",
    "#     return np.array(label_array)\n",
    "\n",
    "train_data_array = imgpath_list_to_array_list(train_tooth_dataframe.Image_path)\n",
    "train_label_array = np.array(train_tooth_dataframe.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10800, 256, 256, 1)\n",
      "(10800,)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_array.shape)\n",
    "print(train_label_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input,Dropout\n",
    "\n",
    "\n",
    "def my_convolution_model(input_shape, classes):\n",
    "    inputs=Input(input_shape)\n",
    "    x = Conv2D(32, (3, 3), activation='relu')(inputs)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(2048, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(classes)(x)\n",
    "    model = Model(inputs, x)\n",
    "    model.compile(optimizer='adam',\n",
    "          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "          metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "input_shape= (256,256,1)\n",
    "classes    = 3\n",
    "batch_size = 128\n",
    "epochs     = 1\n",
    "\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
    "with mirrored_strategy.scope():\n",
    "    model_convolution = my_convolution_model(input_shape, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_convolution.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_train_rate = 0.9\n",
    "data_to_validation_rate = 0.1\n",
    "train_unms = int(len(train_label_array)*data_to_train_rate)\n",
    "print('Have {} data, {} for training , {} for validation'\n",
    "      .format(len(train_label_array),train_unms,len(train_label_array)-train_unms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_convolution.fit(train_data_array[:train_unms], train_label_array[:train_unms], epochs=epochs, \n",
    "                     validation_data=(train_data_array[train_unms:], train_label_array[train_unms:]),shuffle=True,batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpredict = my_convolution_model(input_shape, classes)\n",
    "\n",
    "weights = model_convolution.get_weights()\n",
    "modelpredict.set_weights(weights)\n",
    "modelpredict.save('/root/notebooks/0858611-2/tooth_xray/model/700_700/image_generate_all_rotate/model_weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model_convolution.evaluate(train_data_array[train_unms:],  train_label_array[train_unms:], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_convolution.evaluate(train_data_array[train_unms:],  train_label_array[train_unms:], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "def recall_accuracy_rate(test_data_array,test_label_array,model):\n",
    "    model_predict =[]\n",
    "    for img in test_data_array:\n",
    "        reshape = img.reshape(1,256,256,1)\n",
    "        image = tf.cast(reshape, tf.float32)\n",
    "        model_predict.append(np.argmax(model.predict(image)))\n",
    "        \n",
    "    matrix = confusion_matrix(test_label_array, model_predict)\n",
    "    matrix_split = np.split(matrix,3,axis=1)\n",
    "    print('column:* is prediction result')\n",
    "    print('row*: is real class')\n",
    "    dict = {\n",
    "            \"class:0\":matrix_split[0].reshape(3),\n",
    "            \"class:1\":matrix_split[1].reshape(3),\n",
    "            \"class:2\":matrix_split[2].reshape(3)\n",
    "           }\n",
    "    \n",
    "    predict_table = pd.DataFrame(dict)\n",
    "    return predict_table,matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(label, confusion_matrix):\n",
    "    col = confusion_matrix[:, label]\n",
    "    return confusion_matrix[label, label] / col.sum()\n",
    "    \n",
    "def recall(label, confusion_matrix):\n",
    "    row = confusion_matrix[label, :]\n",
    "    return confusion_matrix[label, label] / row.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing data\n",
    "predict_table1,matrix1 = recall_accuracy_rate(test_data_array,test_label_array,modelpredict)\n",
    "for i in range(len(predict_table1.columns)):\n",
    "    p = precision(i,matrix1)\n",
    "    r = recall(i,matrix1) \n",
    "    print('The {} precision is {:.4f}% in {} prediction and recall is {:.4f}% in {} sample'\n",
    "          .format(predict_table1.columns[i],p*100,matrix1[:,i].sum(),r*100,matrix1[i, :].sum()))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_table1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext tensorboard\n",
    "file_writer = tf.summary.create_file_writer('/root/notebooks/0858611-2/tf-explain/logs')\n",
    "%tensorboard --logdir '/root/notebooks/0858611-2/tf-explain/logs'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = train_data_array[:train_unms]\n",
    "test_labels = tf.keras.utils.to_categorical(train_label_array[:train_unms], num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of the validation data to examine\n",
    "# Here, we choose 5 elements with one hot encoded label \"0\" == [1, 0, 0, .., 0]\n",
    "validation_class_zero = (np.array([\n",
    "    el for el, label in zip(test_images, test_labels)\n",
    "    if np.all(np.argmax(label) == 0)\n",
    "][0:5]), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of the validation data to examine\n",
    "# Here, we choose 5 elements with one hot encoded label \"4\" == [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "validation_class_fours = (np.array([\n",
    "    el for el, label in zip(test_images, test_labels)\n",
    "    if np.all(np.argmax(label) == 4)\n",
    "][0:5]), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate callbacks\n",
    "# class_index value should match the validation_data selected above\n",
    "callbacks = [\n",
    "    tf_explain.callbacks.GradCAMCallback(validation_class_zero, layer_name='target_layer', class_index=0),\n",
    "    tf_explain.callbacks.GradCAMCallback(validation_class_fours, layer_name='target_layer', class_index=4),\n",
    "    tf_explain.callbacks.ActivationsVisualizationCallback(validation_class_zero, layers_name=['target_layer']),\n",
    "    tf_explain.callbacks.SmoothGradCallback(validation_class_zero, class_index=0, num_samples=15, noise=1.),\n",
    "    tf_explain.callbacks.IntegratedGradientsCallback(validation_class_zero, class_index=0, n_steps=10),\n",
    "    tf_explain.callbacks.VanillaGradientsCallback(validation_class_zero, class_index=0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpredict.fit(test_images,test_labels, epochs=epochs, \n",
    "                     validation_data=(train_data_array[train_unms:], train_label_array[train_unms:]),shuffle=True,batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
